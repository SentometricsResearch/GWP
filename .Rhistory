CovidData <- CovidData[CovidData$category != "Ignorer ce contenu",]
CovidData[, "id"] <- zoo::na.locf(CovidData[, "id"])
CovidData <- CovidData[-nrow(CovidData), ]
CovidData = CovidData[is.na(CovidData$page_title),]
CovidData <- CovidData[, c("id", "questions_fr","category")]
CovidData <- CovidData[complete.cases(CovidData), ]
CovidData <- as.data.frame(CovidData)
QR_list <- split(CovidData, f = as.factor(CovidData[, "id"]))
l_QR_list <- unlist(lapply(QR_list, FUN = nrow))
QR_list <- QR_list[l_QR_list >= MinPerQuestion]
QR <- data.table::rbindlist(QR_list)
QR <- as.data.frame(QR)
parse_spacy = spacyr::spacy_parse(QR$questions_fr)
parse_spacy =  spacyr::entity_consolidate(parse_spacy)
parse_spacy$doc_id = as.numeric(stringr::str_sub(parse_spacy$doc_id,5,20))
token = quanteda::as.tokens(split(parse_spacy$lemma,
parse_spacy$doc_id))
dfm = quanteda::dfm(token)
dfm = quanteda::dfm_tfidf(dfm,scheme_tf = "prop",scheme_df = "inverse")
cor = quanteda::textstat_simil(dfm, method = "correlation")
h = hclust(as.dist(1-cor))
cluster = cutree(h, k = (1-ErrorPerc)*nrow(QR))
CovidData = googlesheets4::read_sheet(link,range = "source_quebec.ca")
# CovidData = gsheet2tbl(link)
#CovidData[CovidData == "NA"] <- NA
CovidData <- CovidData[!duplicated(CovidData),]
CovidData$category <- zoo::na.locf(CovidData$category)
CovidData <- CovidData[CovidData$category != "Ignorer ce contenu",]
CovidData[, "id"] <- zoo::na.locf(CovidData[, "id"])
CovidData <- CovidData[-nrow(CovidData), ]
CovidData = CovidData[is.na(CovidData$page_title),]
CovidData <- CovidData[, c("id", "questions_fr","category")]
CovidData <- CovidData[complete.cases(CovidData), ]
CovidData <- as.data.frame(CovidData)
QR_list <- split(CovidData, f = as.factor(CovidData[, "id"]))
l_QR_list <- unlist(lapply(QR_list, FUN = nrow))
QR_list <- QR_list[l_QR_list >= MinPerQuestion]
QR <- data.table::rbindlist(QR_list)
QR <- as.data.frame(QR)
parse_spacy = spacyr::spacy_parse(QR$questions_fr)
parse_spacy =  spacyr::entity_consolidate(parse_spacy)
parse_spacy$doc_id = as.numeric(stringr::str_sub(parse_spacy$doc_id,5,20))
token = quanteda::as.tokens(split(parse_spacy$lemma,
parse_spacy$doc_id))
dfm = quanteda::dfm(token)
dfm = quanteda::dfm_tfidf(dfm,scheme_tf = "prop",scheme_df = "inverse")
cor = quanteda::textstat_simil(dfm, method = "correlation")
h = hclust(as.dist(1-cor))
cluster = cutree(h, k = (1-ErrorPerc)*nrow(QR))
cluster2 = cluster
cluster2[is.na(match(cluster,which(table(cluster) !=1)))] = NA
final_res = split(QR, f = cluster2)
N.suspect = sapply(final_res, FUN = function(x) length(unique(x[,1]))-1)
final_res = final_res[N.suspect > 0]
final_res <- lapply(final_res,FUN = function(x) cbind(rbind(x,""),paste0(x$id,collapse = " ")))
final_res = data.table::rbindlist(final_res)
googlesheets4::write_sheet(final_res, ss = link, sheet = "SanityCheck")
rm(list = ls())
require(zoo)
require(gsheet)
require(spacyr)
require(googlesheets4)
require(data.table)
spacy_initialize(model = "fr_core_news_sm")
#tailored to the french dataset for now, obviously need to be adjusted to be more general
DetectDup = function(ErrorPerc, link, MinPerQuestion) {
CovidData = googlesheets4::read_sheet(link,range = "source_quebec.ca")
# CovidData = gsheet2tbl(link)
#CovidData[CovidData == "NA"] <- NA
CovidData <- CovidData[!duplicated(CovidData),]
CovidData$category <- zoo::na.locf(CovidData$category)
CovidData <- CovidData[CovidData$category != "Ignorer ce contenu",]
CovidData[, "id"] <- zoo::na.locf(CovidData[, "id"])
CovidData <- CovidData[-nrow(CovidData), ]
CovidData = CovidData[is.na(CovidData$page_title),]
CovidData <- CovidData[, c("id", "questions_fr","category")]
CovidData <- CovidData[complete.cases(CovidData), ]
CovidData <- as.data.frame(CovidData)
QR_list <- split(CovidData, f = as.factor(CovidData[, "id"]))
l_QR_list <- unlist(lapply(QR_list, FUN = nrow))
QR_list <- QR_list[l_QR_list >= MinPerQuestion]
QR <- data.table::rbindlist(QR_list)
QR <- as.data.frame(QR)
parse_spacy = spacyr::spacy_parse(QR$questions_fr)
parse_spacy =  spacyr::entity_consolidate(parse_spacy)
parse_spacy$doc_id = as.numeric(stringr::str_sub(parse_spacy$doc_id,5,20))
token = quanteda::as.tokens(split(parse_spacy$lemma,
parse_spacy$doc_id))
dfm = quanteda::dfm(token)
dfm = quanteda::dfm_tfidf(dfm,scheme_tf = "prop",scheme_df = "inverse")
cor = quanteda::textstat_simil(dfm, method = "correlation")
h = hclust(as.dist(1-cor))
cluster = cutree(h, k = (1-ErrorPerc)*nrow(QR))
cluster2 = cluster
cluster2[is.na(match(cluster,which(table(cluster) !=1)))] = NA
final_res = split(QR, f = cluster2)
N.suspect = sapply(final_res, FUN = function(x) length(unique(x[,1]))-1)
final_res = final_res[N.suspect > 0]
final_res <- lapply(final_res,FUN = function(x) cbind(rbind(x,""),paste0(x$id,collapse = " ")))
final_res = data.table::rbindlist(final_res)
googlesheets4::write_sheet(final_res, ss = link, sheet = "SanityCheck")
return(final_res)
}
MinPerQuestion = 3
ErrorPerc = 0.1
link = '1TccSi142sFjek-mhbW_2FDId2rn2Vk5t0rTGKnERViA'
res = DetectDup(ErrorPerc = ErrorPerc,  MinPerQuestion = MinPerQuestion,
link = link)
#tailored to the french dataset for now, obviously need to be adjusted to be more general
DetectDup = function(ErrorPerc, link, MinPerQuestion) {
CovidData = googlesheets4::read_sheet(link,range = "source_quebec.ca")
# CovidData = gsheet2tbl(link)
#CovidData[CovidData == "NA"] <- NA
CovidData <- CovidData[!duplicated(CovidData),]
CovidData$category <- zoo::na.locf(CovidData$category)
CovidData <- CovidData[CovidData$category != "Ignorer ce contenu",]
CovidData[, "id"] <- zoo::na.locf(CovidData[, "id"])
CovidData <- CovidData[-nrow(CovidData), ]
CovidData = CovidData[is.na(CovidData$page_title),]
CovidData <- CovidData[, c("id", "questions_fr","category")]
CovidData <- CovidData[complete.cases(CovidData), ]
CovidData <- as.data.frame(CovidData)
QR_list <- split(CovidData, f = as.factor(CovidData[, "id"]))
l_QR_list <- unlist(lapply(QR_list, FUN = nrow))
QR_list <- QR_list[l_QR_list >= MinPerQuestion]
QR <- data.table::rbindlist(QR_list)
QR <- as.data.frame(QR)
parse_spacy = spacyr::spacy_parse(QR$questions_fr)
parse_spacy =  spacyr::entity_consolidate(parse_spacy)
parse_spacy$doc_id = as.numeric(stringr::str_sub(parse_spacy$doc_id,5,20))
token = quanteda::as.tokens(split(parse_spacy$lemma,
parse_spacy$doc_id))
dfm = quanteda::dfm(token)
dfm = quanteda::dfm_tfidf(dfm,scheme_tf = "prop",scheme_df = "inverse")
cor = quanteda::textstat_simil(dfm, method = "correlation")
h = hclust(as.dist(1-cor))
cluster = cutree(h, k = (1-ErrorPerc)*nrow(QR))
cluster2 = cluster
cluster2[is.na(match(cluster,which(table(cluster) !=1)))] = NA
final_res = split(QR, f = cluster2)
N.suspect = sapply(final_res, FUN = function(x) length(unique(x[,1]))-1)
final_res = final_res[N.suspect > 0]
final_res <- lapply(final_res,FUN = function(x) cbind(rbind(x,""),paste0(x$questions_fr,collapse = ";;;;;;")))
final_res = data.table::rbindlist(final_res)
googlesheets4::write_sheet(final_res, ss = link, sheet = "SanityCheck")
return(final_res)
}
MinPerQuestion = 3
ErrorPerc = 0.4
CovidData = googlesheets4::read_sheet(link,range = "source_quebec.ca")
# CovidData = gsheet2tbl(link)
#CovidData[CovidData == "NA"] <- NA
CovidData <- CovidData[!duplicated(CovidData),]
CovidData$category <- zoo::na.locf(CovidData$category)
CovidData <- CovidData[CovidData$category != "Ignorer ce contenu",]
CovidData[, "id"] <- zoo::na.locf(CovidData[, "id"])
CovidData <- CovidData[-nrow(CovidData), ]
CovidData = CovidData[is.na(CovidData$page_title),]
CovidData <- CovidData[, c("id", "questions_fr","category")]
CovidData <- CovidData[complete.cases(CovidData), ]
CovidData <- as.data.frame(CovidData)
QR_list <- split(CovidData, f = as.factor(CovidData[, "id"]))
l_QR_list <- unlist(lapply(QR_list, FUN = nrow))
QR_list <- QR_list[l_QR_list >= MinPerQuestion]
QR <- data.table::rbindlist(QR_list)
QR <- as.data.frame(QR)
parse_spacy = spacyr::spacy_parse(QR$questions_fr)
parse_spacy =  spacyr::entity_consolidate(parse_spacy)
parse_spacy$doc_id = as.numeric(stringr::str_sub(parse_spacy$doc_id,5,20))
token = quanteda::as.tokens(split(parse_spacy$lemma,
parse_spacy$doc_id))
dfm = quanteda::dfm(token)
dfm = quanteda::dfm_tfidf(dfm,scheme_tf = "prop",scheme_df = "inverse")
cor = quanteda::textstat_simil(dfm, method = "correlation")
h = hclust(as.dist(1-cor))
cluster = cutree(h, k = (1-ErrorPerc)*nrow(QR))
cluster2 = cluster
cluster2[is.na(match(cluster,which(table(cluster) !=1)))] = NA
final_res = split(QR, f = cluster2)
N.suspect = sapply(final_res, FUN = function(x) length(unique(x[,1]))-1)
final_res = final_res[N.suspect > 0]
final_res <- lapply(final_res,FUN = function(x) cbind(rbind(x,""),paste0(x$id," - ",x$questions_fr,collapse = ";;;;;;")))
final_res[,1]
final_res[1,]
final_res = data.table::rbindlist(final_res)
final_res[1,]
colnames(final_res) = c("id","questions_fr","similar")
dim(final_res)
colnames(final_res) = c("id","questions_fr","category","similar")
#tailored to the french dataset for now, obviously need to be adjusted to be more general
DetectDup = function(ErrorPerc, link, MinPerQuestion) {
CovidData = googlesheets4::read_sheet(link,range = "source_quebec.ca")
# CovidData = gsheet2tbl(link)
#CovidData[CovidData == "NA"] <- NA
CovidData <- CovidData[!duplicated(CovidData),]
CovidData$category <- zoo::na.locf(CovidData$category)
CovidData <- CovidData[CovidData$category != "Ignorer ce contenu",]
CovidData[, "id"] <- zoo::na.locf(CovidData[, "id"])
CovidData <- CovidData[-nrow(CovidData), ]
CovidData = CovidData[is.na(CovidData$page_title),]
CovidData <- CovidData[, c("id", "questions_fr","category")]
CovidData <- CovidData[complete.cases(CovidData), ]
CovidData <- as.data.frame(CovidData)
QR_list <- split(CovidData, f = as.factor(CovidData[, "id"]))
l_QR_list <- unlist(lapply(QR_list, FUN = nrow))
QR_list <- QR_list[l_QR_list >= MinPerQuestion]
QR <- data.table::rbindlist(QR_list)
QR <- as.data.frame(QR)
parse_spacy = spacyr::spacy_parse(QR$questions_fr)
parse_spacy =  spacyr::entity_consolidate(parse_spacy)
parse_spacy$doc_id = as.numeric(stringr::str_sub(parse_spacy$doc_id,5,20))
token = quanteda::as.tokens(split(parse_spacy$lemma,
parse_spacy$doc_id))
dfm = quanteda::dfm(token)
dfm = quanteda::dfm_tfidf(dfm,scheme_tf = "prop",scheme_df = "inverse")
cor = quanteda::textstat_simil(dfm, method = "correlation")
h = hclust(as.dist(1-cor))
cluster = cutree(h, k = (1-ErrorPerc)*nrow(QR))
cluster2 = cluster
cluster2[is.na(match(cluster,which(table(cluster) !=1)))] = NA
final_res = split(QR, f = cluster2)
N.suspect = sapply(final_res, FUN = function(x) length(unique(x[,1]))-1)
final_res = final_res[N.suspect > 0]
final_res <- lapply(final_res,FUN = function(x) cbind(rbind(x,""),paste0(x$id," - ",x$questions_fr,collapse = " ;;;;;; ")))
final_res = data.table::rbindlist(final_res)
colnames(final_res) = c("id","questions_fr","category","similar")
googlesheets4::write_sheet(final_res, ss = link, sheet = "SanityCheck")
return(final_res)
}
MinPerQuestion = 3
ErrorPerc = 0.2
link = '1TccSi142sFjek-mhbW_2FDId2rn2Vk5t0rTGKnERViA'
res = DetectDup(ErrorPerc = ErrorPerc,  MinPerQuestion = MinPerQuestion,
link = link)
rm(list = ls())
require(zoo)
require(zoo)
require(gsheet)
require(spacyr)
require(data.table)
spacy_initialize(model = "fr_core_news_sm")
# Function that detect Close questions for Covid-19 bot project using simple text processing and tf idf similarlty.
# Note that manual check should be perform, this function generate candidate to decrease the working load.
# MinPerQuestion (scalar) Minimum number of question by ID.
# ErrorPerc (float between 1 and -1)  Tree cut, the  higher, the less aggresive the detection is.
# link (string) link to google sheet
#tailored to the french dataset for now, obviously need to be adjusted to be more general
DetectDup = function(ErrorPerc, link, MinPerQuestion) {
CovidData = gsheet2tbl(link)
CovidData[CovidData == ""] <- NA
CovidData <- CovidData[!duplicated(CovidData),]
CovidData$category <- zoo::na.locf(CovidData$category)
CovidData <- CovidData[CovidData$category != "Ignorer ce contenu",]
CovidData[, "id"] <- zoo::na.locf(CovidData[, "id"])
CovidData <- CovidData[-nrow(CovidData), ]
CovidData = CovidData[is.na(CovidData$page_title),]
CovidData <- CovidData[, c("id", "questions_fr","category")]
CovidData <- CovidData[complete.cases(CovidData), ]
CovidData <- as.data.frame(CovidData)
QR_list <- split(CovidData, f = as.factor(CovidData[, "id"]))
l_QR_list <- unlist(lapply(QR_list, FUN = nrow))
QR_list <- QR_list[l_QR_list >= MinPerQuestion]
QR <- data.table::rbindlist(QR_list)
QR <- as.data.frame(QR)
parse_spacy = spacyr::spacy_parse(QR$questions_fr)
parse_spacy =  spacyr::entity_consolidate(parse_spacy)
parse_spacy$doc_id = as.numeric(stringr::str_sub(parse_spacy$doc_id,5,20))
token = quanteda::as.tokens(split(parse_spacy$lemma,
parse_spacy$doc_id))
dfm = quanteda::dfm(token)
dfm = quanteda::dfm_tfidf(dfm,scheme_tf = "prop",scheme_df = "inverse")
cor = quanteda::textstat_simil(dfm, method = "correlation")
h = hclust(as.dist(1-cor))
cluster = cutree(h, k = (1-ErrorPerc)*nrow(QR))
cluster2 = cluster
cluster2[is.na(match(cluster,which(table(cluster) !=1)))] = NA
final_res = split(QR, f = cluster2)
N.suspect = sapply(final_res, FUN = function(x) length(unique(x[,1]))-1)
final_res = final_res[N.suspect > 0]
final_res <- lapply(final_res,FUN = function(x) rbind(x,""))
final_res = data.table::rbindlist(final_res)
return(final_res)
}
MinPerQuestion = 3
ErrorPerc = 0.4
link = 'docs.google.com/spreadsheets/d/1of-lCN-SWP7cI9OoofWVcz_iMpwI3IS--lwpj6Grfik/edit?ts=5e8b50d1#gid=1474666511'
res = DetectDup(ErrorPerc = ErrorPerc,  MinPerQuestion = MinPerQuestion,
link = link)
res
rm(list = ls())
require(zoo)
require(zoo)
require(gsheet)
require(spacyr)
require(data.table)
spacy_initialize(model = "fr_core_news_sm")
# Function that detect Close questions for Covid-19 bot project using simple text processing and tf idf similarlty.
# Note that manual check should be perform, this function generate candidate to decrease the working load.
# MinPerQuestion (scalar) Minimum number of question by ID.
# ErrorPerc (float between 1 and -1)  Tree cut, the  higher, the less aggresive the detection is.
# link (string) link to google sheet
#tailored to the french dataset for now, obviously need to be adjusted to be more general
DetectDup = function(ErrorPerc, link, MinPerQuestion) {
CovidData = gsheet2tbl(link)
CovidData[CovidData == ""] <- NA
CovidData <- CovidData[!duplicated(CovidData),]
CovidData$category <- zoo::na.locf(CovidData$category)
CovidData <- CovidData[CovidData$category != "Ignorer ce contenu",]
CovidData[, "id"] <- zoo::na.locf(CovidData[, "id"])
CovidData <- CovidData[-nrow(CovidData), ]
CovidData = CovidData[is.na(CovidData$page_title),]
CovidData <- CovidData[, c("id", "questions_fr","category")]
CovidData <- CovidData[complete.cases(CovidData), ]
CovidData <- as.data.frame(CovidData)
QR_list <- split(CovidData, f = as.factor(CovidData[, "id"]))
l_QR_list <- unlist(lapply(QR_list, FUN = nrow))
QR_list <- QR_list[l_QR_list >= MinPerQuestion]
QR <- data.table::rbindlist(QR_list)
QR <- as.data.frame(QR)
parse_spacy = spacyr::spacy_parse(QR$questions_fr)
#parse_spacy =  spacyr::entity_consolidate(parse_spacy)
parse_spacy$doc_id = as.numeric(stringr::str_sub(parse_spacy$doc_id,5,20))
token = quanteda::as.tokens(split(parse_spacy$lemma,
parse_spacy$doc_id))
dfm = quanteda::dfm(token)
dfm = quanteda::dfm_tfidf(dfm,scheme_tf = "prop",scheme_df = "inverse")
cor = quanteda::textstat_simil(dfm, method = "correlation")
h = hclust(as.dist(1-cor))
cluster = cutree(h, k = (1-ErrorPerc)*nrow(QR))
cluster2 = cluster
cluster2[is.na(match(cluster,which(table(cluster) !=1)))] = NA
final_res = split(QR, f = cluster2)
N.suspect = sapply(final_res, FUN = function(x) length(unique(x[,1]))-1)
final_res = final_res[N.suspect > 0]
final_res <- lapply(final_res,FUN = function(x) rbind(x,""))
final_res = data.table::rbindlist(final_res)
return(final_res)
}
MinPerQuestion = 3
ErrorPerc = 0.2
link = 'docs.google.com/spreadsheets/d/1of-lCN-SWP7cI9OoofWVcz_iMpwI3IS--lwpj6Grfik/edit?ts=5e8b50d1#gid=1474666511'
res = DetectDup(ErrorPerc = ErrorPerc,  MinPerQuestion = MinPerQuestion,
link = link)
res
spacy_initialize(model = "fr_core_news_md")
spacy_download_langmodel("fr_core_news_md")
spacy_initialize(model = "fr_core_news_md")
str("ssdf")
Encoding("asd")
"/"
"xasd/asda" === "xasd/asda"
"xasd/asda" == "xasd/asda"
"asdfas      asdfasf"
"asdfas        asdfasf"
x= rnorm(10000,0,1)^2
mean(x[x<0])
x
mean(x[x<0])
x= rnorm(10000,0,1)
mean(x^2[x<0])
x= rnorm(10000,0,1)
mean(x[x<0]^2)
x= rnorm(10000,0,1)
mean(x[x<0]^2)
mean(x^2*x<0)
x= rt(10000,0,1)
mean(x^2*x<0)
x= rt(n = 10000,df = 4)
mean(x^2*x<0)
x= rt(n = 10000,df = 4)
mean(x^2*x<0)
x= rt(n = 10000,df = 100)
mean(x^2*x<0)
mvtnorm::rmvnorm(768, mean = 0,sigma = matrix(c(1,0.5,0.5,1),nrow = 2,ncol = 2)
mvtnorm::rmvnorm(768, mean = 0,sigma = matrix(c(1,0.5,0.5,1),nrow = 2,ncol = 2))
atrix(c(1,0.5,0.5,1),nrow = 2,ncol = 2)
matrix(c(1,0.5,0.5,1),nrow = 2,ncol = 2)
mvtnorm::rmvnorm(768, mean = c(0,0),sigma = matrix(c(1,0.5,0.5,1),nrow = 2,ncol = 2))
x = mvtnorm::rmvnorm(768, mean = c(0,0),sigma = matrix(c(1,0.5,0.5,1),nrow = 2,ncol = 2))
w = rnorm(768,0,1)
sum((x - y)^2)
sum((x - w)^2)
sum((x - w)^2)/768
sqrt(sum((x - w)^2)/768)
y
sqrt(sum((x[,1] - x[,2])^2)/768)
sqrt(sum((x[,1] - x[,2])^2)/768)
cor(x)
dist(x)
dist(t(x))
dist(t(x),method = "eucliendean")
sqrt(sum((x[,1] - x[,2])^2))
(x[,1] - w)
sqrt(sum((x[,1] - w)^2))
sqrt(sum((x[,2] - w)^2))
sqrt(sum((x[,2] - x[,1])^2))
x = mvtnorm::rmvnorm(768, mean = c(0,0),sigma = matrix(c(1,0,0,1),nrow = 2,ncol = 2))
w = rnorm(768,0,1)
sqrt(sum((x[,1] - x[,2])^2))
sqrt(sum((x[,1] - w)^2))
sqrt(sum((x[,2] - w)^2))
sqrt(sum((x[,2] - x[,1])^2))
sqrt(sum((x[,1] - x[,2])^2))
sqrt(sum((x[,1] - w)^2))
sqrt(sum((x[,1] - w)^2))
sqrt(sum((x[,2] - w)^2))
sqrt(sum((x[,2] - x[,1])^2))
exp(-sqrt(sum((x[,1] - w)^2)))
exp(-sqrt(sum((x[,2] - w)^2)))
require(reticulate)
reticulate::install_miniconda()
reticulate::conda_create("reticulate")
reticulate::use_miniconda("reticulate")
reticulate::py_config()
reticulate::py_install("pytorch")
reticulate::py_install("tensorflow", pip = TRUE)
reticulate::py_install("tokenizers",pip = TRUE)
reticulate::py_install("transformers",pip = TRUE)
#instantiate python libraries
torch = reticulate::import("torch")
tf = reticulate::import("tensorflow")
tok = reticulate::import("tokenizers")
trans = reticulate::import("transformers")
reticulate::py_install("transformers",pip = TRUE)
reticulate::py_install("transformers",pip = TRUE)
trans = reticulate::import("transformers")
#check for availability of DeepLearnings framework
trans$is_tf_available()
trans$is_torch_available()
tokenizer = trans$AutoTokenizer$from_pretrained("wietsedv/bert-base-dutch-cased-finetuned-sentiment")
# Instanciate model config from model specification, not always needed, but needed to add output_hidden_states for embedding
config = trans$BertConfig$from_pretrained("wietsedv/bert-base-dutch-cased-finetuned-sentiment", output_hidden_states=TRUE)
# generate model with pre-trained weight from model specification
model = trans$AutoModelForSequenceClassification$from_pretrained("wietsedv/bert-base-dutch-cased-finetuned-sentiment",config=config)
# example with pipeline module of huggingface
pipeline = trans$pipeline(task = 'sentiment-analysis',  model = model, tokenizer = tokenizer)
print(pipeline("ik haat je"))
print(pipeline("hou van jou"))
# example on how to encode text
e.text = tokenizer$encode("ik haat je")
unlist(tokenizer$get_vocab())[e.text+1]
text = torch$tensor(e.text)
text = text$unsqueeze(0L)
#instantiate python libraries
torch = reticulate::import("torch")
# example on how to encode text
e.text = tokenizer$encode("ik haat je")
unlist(tokenizer$get_vocab())[e.text+1]
text = torch$tensor(e.text)
text = text$unsqueeze(0L)
# generate model output based on text
outputs = model(text)
#extract embedding to the text
data.table::rbindlist(outputs[[2]][[1]]$tolist())
# example on how to encode text
e.text = tokenizer$encode("hou van jou")
unlist(tokenizer$get_vocab())[e.text2+1]
# example on how to encode text
e.text = tokenizer$encode("hou van jou")
unlist(tokenizer$get_vocab())[e.text+1]
text = torch$tensor(e.text)
text = text$unsqueeze(0L)
e.text2 = tokenizer$encode("ik haat je")
unlist(tokenizer$get_vocab())[e.text2+1]
text = torch$tensor(e.text2)
e.text2 = tokenizer$encode("ik haat je")
unlist(tokenizer$get_vocab())[e.text2+1]
text2 = torch$tensor(e.text2)
text2 = text$unsqueeze(0L)
# generate model output based on text
outputs1 = model(text)
# example on how to encode text
e.text = tokenizer$encode("hou van jou")
unlist(tokenizer$get_vocab())[e.text+1]
text = torch$tensor(e.text)
text = text$unsqueeze(0L)
e.text2 = tokenizer$encode("ik haat je")
unlist(tokenizer$get_vocab())[e.text2+1]
text2 = torch$tensor(e.text2)
text2 = text$unsqueeze(0L)
# generate model output based on text
outputs1 = model(text2)
# generate model output based on text
outputs1 = model(text)
outputs2 = model(text2)
e.text2 = tokenizer$encode("ik haat je")
unlist(tokenizer$get_vocab())[e.text2+1]
text2 = torch$tensor(e.text2)
text2 = text2$unsqueeze(0L)
# generate model output based on text
outputs1 = model(text)
outputs2 = model(text2)
#extract embedding to the text
data.table::rbindlist(outputs[[2]][[1]]$tolist())
#extract embedding to the text
emb1 = data.table::rbindlist(outputs1[[2]][[1]]$tolist())
emb2 = data.table::rbindlist(outputs2[[2]][[1]]$tolist())
emb1
emb2
emb1
unlist(tokenizer$get_vocab())[e.text2+1]
table(unlist(tokenizer$get_vocab())[e.text2+1])
#extract embedding to the text
emb1 = data.table::rbindlist(outputs1[[2]][[1]]$tolist())[,c(-1,-length(e.text1))]
emb2 = data.table::rbindlist(outputs2[[2]][[1]]$tolist())[,c(-1,-length(e.text2))]
#extract embedding to the text
emb1 = data.table::rbindlist(outputs1[[2]][[1]]$tolist())[,c(-1,-length(e.text))]
emb2 = data.table::rbindlist(outputs2[[2]][[1]]$tolist())[,c(-1,-length(e.text2))]
emb1
setwd("~/GitHub/GWP")
devtools::install()
devtools::install()
